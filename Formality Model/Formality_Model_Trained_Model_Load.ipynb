{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf55d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports stuff and installs stuff takes a few secs to run\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import csv\n",
    "import numpy as np\n",
    "import spacy\n",
    "import sklearn\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "string_punctuation = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "stop_words = set(stopwords.words('english'))\n",
    "!python -m spacy download en_core_web_md\n",
    "\n",
    "# Spacy model imported\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21089ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input anvil sentence text in sentences list\n",
    "sentences = [\"How's it going?\",\n",
    "            \"It is our honest opinion that our organization is not properly positioned to invest in that platform.\",\n",
    "            \"We have reviewed your application and, unfortunately, we have decided to move forward with another applicant for this position.\",\n",
    "            \"Hello I am very nice to meet you!\",\n",
    "            \"I need help with my project.\"]\n",
    "\n",
    "#Converts that to a dataframe\n",
    "data = pd.DataFrame(sentences,columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be9428a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Active/passive voice\n",
    "\n",
    "# function to check the type of sentence\n",
    "# 0 refers to passive, 1 refers to active\n",
    "def checkForSentType(inputSentence):   \n",
    "    # running the model on sentence\n",
    "    getDocFile = nlp(inputSentence)\n",
    "    \n",
    "    # getting the syntactic dependency \n",
    "    getAllTags = [token.dep_ for token in getDocFile]\n",
    "    \n",
    "    # checking for 'agent' tag\n",
    "    checkPassiveTest = any(['agent' in sublist for sublist in getAllTags])\n",
    "    \n",
    "    # checking for 'nsubjpass' tag\n",
    "    checkPassiveTestTwo = any(['nsubjpass' in sublist for sublist in getAllTags])\n",
    "    if checkPassiveTest or checkPassiveTestTwo:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "#Importing the list of formal/informal words\n",
    "formal_words = pd.read_excel('formal_words.xlsx',header=None)\n",
    "formal_list = formal_words[0].values.tolist()\n",
    "informal_words = pd.read_excel('informal_words.xlsx',header=None)\n",
    "informal_list = formal_words[0].values.tolist()\n",
    "\n",
    "\n",
    "#Formal Pronouns\n",
    "#Often third person\n",
    "Formal_Pronouns = [\n",
    "    \"one\",\"oneself\",\"one's\", \n",
    "    \"who\",\"whom\",\"whomst\",\"whose\",\n",
    "    \"they\",\"them\",\"their\",\"theirs\",\"themself\",\"themselves\",\"theirself\",\"theirselves\",\n",
    "    \"it\",\"its\",\"itself\",\n",
    "    \"he\",\"him\",\"himself\",\"his\",\n",
    "    \"she\",\"her\",\"herself\",\"hers\"\n",
    "]\n",
    "\n",
    "#Informal Pronouns\n",
    "#Often first person\n",
    "Informal_Pronouns = [\n",
    "    \"I\",\"me\",\"mine\",\"mines\",\"my\",\"myself\",\n",
    "    \"we\",\"us\",\"ourself\",\"ourselves\",\"our\",\"ours\",\n",
    "    \"you\",\"your\",\"yourself\",\"yours\",\"yourselves\",\"y'all\",\"yall\",\"y'all's\"\n",
    "]\n",
    "\n",
    "#Feature Extracter\n",
    "def anvil_cleaner(data):\n",
    "            \n",
    "    #wored count\n",
    "    data['Word Count']= data[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    #Counting number of formal pronouns\n",
    "    data['Formal Pronoun'] = data[\"text\"].apply(\n",
    "        lambda x:len([w for w in str(x).lower().split() if w in Formal_Pronouns]))/data['Word Count']\n",
    "        \n",
    "    #Counting number of informal pronouns\n",
    "    data['Informal Pronoun'] = data[\"text\"].apply(\n",
    "        lambda x:len([w for w in str(x).lower().split() if w in Informal_Pronouns]))/data['Word Count']\n",
    "    \n",
    "    #Counting number of contractions\n",
    "    data['Contractions'] = data[\"text\"].apply(lambda x: x.count(\"'\"))/data['Word Count']\n",
    "    \n",
    "    #Identifying sentences with active voice\n",
    "    data['Active Voice'] = data['text'].apply(checkForSentType)\n",
    "    \n",
    "    #Identifying sentences with passive voice (just opposite of active, since all sentences are active or passive)\n",
    "    data['Passive Voice'] = 1 - data['text'].apply(checkForSentType)\n",
    "        \n",
    "    # Removing apostrophes so contractions are considered a single token\n",
    "    data['clean_text'] = data['text'].apply(lambda x: re.sub(\"'\", '', x))\n",
    "    \n",
    "    # Remove punctuation and stop words and lowercase the text\n",
    "    data['clean_text'] = data['clean_text'].apply(lambda x: ' '.join(re.sub(\n",
    "        r'[^a-zA-Z]',' ',w).lower() for w in x.split() if re.sub(r'[^a-zA-Z]',' ',w).lower() not in stop_words))\n",
    "\n",
    "    #lemmatize\n",
    "    #data['clean_text'] = data['clean_text'].apply(lambda x: ' '.join(lemmatizer.lemmatize(w) for w in x.split()))\n",
    "\n",
    "\n",
    "    #now engineer the features the model expects\n",
    "    \n",
    "    #Formal words count (normalized to sentence length)\n",
    "    #data['Formal Words'] = data[\"text\"].apply(\n",
    "    #    lambda x:len([w for w in str(x).split() if w in formal_list]))/data['Word Count']\n",
    "        \n",
    "    #Informal words count (normalized to sentence length)\n",
    "    #data['Informal Words'] = data[\"text\"].apply(\n",
    "    #    lambda x:len([w for w in str(x).split() if w in informal_list]))/data['Word Count']\n",
    "        \n",
    "    #wored count\n",
    "    data['Word Count']= data[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "\n",
    "    data['Character count'] = data[\"text\"].apply(lambda x: len(str(x))) \n",
    "\n",
    "    data[\"average characters per word\"] =  data['Character count']/data['Word Count']\n",
    "\n",
    "    data['stopword count'] = data[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stop_words]))/data['Word Count']\n",
    "\n",
    "\n",
    "    #feature for all the nowns in a text \n",
    "    from nltk import word_tokenize\n",
    "    all_text_without_sw = ''\n",
    "    for i in data.itertuples():\n",
    "        all_text_without_sw = all_text_without_sw +  str(i.text)\n",
    "\n",
    "    tokenized_all_text = word_tokenize(all_text_without_sw) #tokenize the text\n",
    "    list_of_tagged_words = nltk.pos_tag(tokenized_all_text) #adding POS Tags to tokenized words\n",
    "\n",
    "    set_pos  = (set(list_of_tagged_words)) # set of POS tags & words\n",
    "\n",
    "    nouns = ['NN','NNS','NNP','NNPS'] #POS tags of nouns\n",
    "    list_of_words = set(map(lambda tuple_2 : tuple_2[0], filter(lambda tuple_2 : tuple_2[1] in  nouns, set_pos)))\n",
    "    #data['noun count'] = data['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]) )/data['Word Count']\n",
    "\n",
    "    # prnown count\n",
    "\n",
    "    pronouns = ['PRP','PRP$','WP','WP$'] # POS tags of pronouns\n",
    "    list_of_words = set(map(lambda tuple_2 : tuple_2[0], filter(lambda tuple_2 : tuple_2[1] in  pronouns, set_pos)))\n",
    "    #data['pronoun count'] = data['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]) )/data['Word Count']\n",
    "\n",
    "    # count fo verbs\n",
    "\n",
    "    verbs = ['VB','VBD','VBG','VBN','VBP','VBZ'] #POS tags of verbs\n",
    "    list_of_words = set(map(lambda tuple_2 : tuple_2[0], filter(lambda tuple_2 : tuple_2[1] in  verbs, set_pos)))\n",
    "    #data['verb count'] = data['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]) )/data['Word Count']\n",
    "\n",
    "\n",
    "    #adverb count\n",
    "\n",
    "    adverbs = ['RB','RBR','RBS','WRB'] #POS tags of adverbs\n",
    "    list_of_words = set(map(lambda tuple_2 : tuple_2[0], filter(lambda tuple_2 : tuple_2[1] in  adverbs, set_pos)))\n",
    "    data['adverb count'] = data['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]) )/data['Word Count']\n",
    "\n",
    "    #Adjective count\n",
    "\n",
    "    adjectives = ['JJ','JJR','JJS'] #POS tags of adjectives\n",
    "    list_of_words = set(map(lambda tuple_2 : tuple_2[0], filter(lambda tuple_2 : tuple_2[1] in  adjectives, set_pos)))\n",
    "    #data['adjective count'] = data['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]) )/data['Word Count']\n",
    "\n",
    "\n",
    "    data['punctuation count'] = data['clean_text'].apply(lambda x: len([w for w in str(x) if w in string_punctuation]))/data['Word Count']\n",
    "\n",
    "\n",
    "    data['mean sentance length'] = data['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55376fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hansonqin/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.24.2 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Load in trained pickle model (Logistic regression)\n",
    "with open('formality_model.pkl', 'rb') as f:\n",
    "    lr_loaded = pickle.load(f)\n",
    "clean_data = anvil_cleaner(data)\n",
    "\n",
    "#Transforms data to be inserted into model (drops text and scales)\n",
    "clean_data = clean_data.drop(['text', 'clean_text'], axis=1)\n",
    "cols = clean_data.columns\n",
    "scaler = MinMaxScaler()\n",
    "clean_data = scaler.fit_transform(clean_data)\n",
    "clean_data = pd.DataFrame(clean_data, columns=[cols])\n",
    "\n",
    "#Get probabilities of formalities (0 is informal, 1 is formal)\n",
    "lr_loaded.predict_proba(clean_data)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f203e31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
