{"cells":[{"cell_type":"markdown","id":"d3ed8c8c","metadata":{"id":"d3ed8c8c"},"source":["# Kriging method"]},{"cell_type":"code","execution_count":null,"id":"55d3fa98","metadata":{"id":"55d3fa98"},"outputs":[],"source":["# PYTHON STANDARD LIBRARY\n","import json\n","import io\n","import zipfile\n","\n","from requests import get, post, put, delete\n","from requests.auth import HTTPBasicAuth\n","\n","from pprint import pprint\n","\n","# 3RD PARTY DATA SCIENCE\n","import pandas as pd\n","import numpy as np \n","\n","# GEOSPATIAL\n","import geopandas as gpd\n","import shapely\n","from shapely import wkt\n","import fiona\n","import seaborn as sns\n","\n","from dateutil.relativedelta import *\n","from dateutil.easter import *\n","from dateutil.rrule import *\n","from dateutil.parser import *\n","from datetime import *\n","import pyproj    \n","import shapely\n","import shapely.ops as ops\n","from shapely.geometry.polygon import Polygon, Point\n","from functools import partial\n","import matplotlib.pyplot as plt\n","import random\n","from scipy.spatial import distance as dt\n","import mapclassify\n","import tqdm\n","import os\n","\n","from matplotlib import pyplot\n","from shapely.geometry.polygon import LinearRing, Polygon\n","from sklearn.metrics import mean_absolute_percentage_error"]},{"cell_type":"markdown","metadata":{"id":"z0T-2OQ8bvT5"},"source":["---\n","# Import data from API"],"id":"z0T-2OQ8bvT5"},{"cell_type":"markdown","metadata":{"id":"hUTE88nxbvT8"},"source":["We are only using the API in this notebook for getting date/time list for the training and validation data, not the shapefiles themselves.\n","\n","For getting the actual data please use the `aoi_data_by_time` notebook."],"id":"hUTE88nxbvT8"},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"eUH-ZFUjbvT9"},"outputs":[],"source":["token = input('Token >>  ');"],"id":"eUH-ZFUjbvT9"},{"cell_type":"code","execution_count":null,"metadata":{"id":"EQUT3AhVbvUA"},"outputs":[],"source":["# SET THE API URL AS A LOCAL VARIABLE (IT WILL BE USED A LOT!)\n","api = 'https://go-services.orbitalinsight.com/api/v2/go'\n","# DEFINE THE HEADERS\n","headers = {\"Content-Type\": \"application/json\", \n","           \"X-Orbitalinsight-Auth-Token\": token}\n","# DEFINE THE PROJECT AND VERSION IDS\n","pid = 'DzRmN00uWW-210911'\n","vid = 'DzRmN00uWW-210911-1.0.0'\n","# MODIFY THE API URL\n","vid_url = f'{api}/projects/{pid}/versions/{vid}'\n","\n","# PERFORM THE GET COMMAND\n","response = get(vid_url, headers = headers)\n","# GENERATE A DICT AND CONVERT TO JSON\n","data_dict = {\"detail\": \"false\"}\n","data_json = json.dumps(data_dict)\n","\n","# MODIFY THE API URL\n","dl_request_url = f'{api}/projects/{pid}/versions/{vid}/results/download'\n","\n","# PERFORM THE POST COMMAND\n","response = post(dl_request_url, data = data_json, headers = headers)\n","\n","# EXTRACT THE DOWNLOAD ID FROM THE RESPONSE \n","download_id = response.json()['data']['results_download_id']\n","# MODIFY THE API URL\n","dl_url = f'{api}/projects/{pid}/versions/{vid}/results/download?results_download_id={download_id}' \n","\n","# PERFORM THE GET COMMAND\n","response = get(dl_url, headers = headers)\n","\n","# GET THE DOWNLOAD URL IF THE DOWNLOAD IS READY\n","if 'Result is ready' in response.json()['data']['message']:\n","    download_url = response.json()['data']['url']\n","# PERFORM THE GET COMMAND\n","resp = get(download_url)\n","# PARSE THE ZIP DATA FROM THE HTTP RESPONSE INTO A PANDAS DATAFRAME\n","with zipfile.ZipFile(io.BytesIO(resp.content)) as z:\n","    with z.open(z.namelist()[0]) as f:\n","        results = pd.read_csv(f)"],"id":"EQUT3AhVbvUA"},{"cell_type":"markdown","source":["### Filter out days with high cloud coverage"],"metadata":{"id":"XoncrBZPClDq"},"id":"XoncrBZPClDq"},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"dTrtSI2nbvUC"},"outputs":[],"source":["def rule_out(level):\n","    \"\"\"\n","    Filter the clouds with less than a certain cloud coverage `level`\n","\n","    Returns: DataFrame object\n","    \"\"\"\n","    results_low_cloud_coverage = results[(results[\"OI Cloud Coverage\"] < level)]\n","    return results_low_cloud_coverage"],"id":"dTrtSI2nbvUC"},{"cell_type":"code","execution_count":null,"metadata":{"id":"l0_N4rKvbvUE"},"outputs":[],"source":["# Get all the date time with cloud coverage less than 0.1\n","data = rule_out(0.1)\n","# Name list for all AOI\n","aoi_list = rule_out(0.1)['AOI name'].unique()\n","aoi_list"],"id":"l0_N4rKvbvUE"},{"cell_type":"code","execution_count":null,"metadata":{"id":"norWqKbtbvUF"},"outputs":[],"source":["# Of all the dates with cloud coverage less than 0.1, we use those with cloud coverage less than 0.01\n","data['imaging_date'] = data['Measurement Timestamp (UTC)'].str.split(pat=' ', expand=True)[0]\n","data = data[data['AOI name'] == 'Aktubinsk']\n","train = list(data[data['OI Cloud Coverage'] > 0.01]['Measurement Timestamp (UTC)'].unique())\n","validation = list(data[data['OI Cloud Coverage'] <= 0.01]['Measurement Timestamp (UTC)'].unique())"],"id":"norWqKbtbvUF"},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"bFlZJByNbvUH"},"outputs":[],"source":["# Set a dictionary for all AOIs and their corresponding training/validaton dates\n","train = {}\n","validation = {}\n","for aoi in aoi_list:\n","    data = rule_out(0.1)\n","    data[data['AOI name'] == aoi]\n","    train[aoi] = list(data[data['OI Cloud Coverage'] > 0.01]['Measurement Timestamp (UTC)'].unique())\n","    validation[aoi] = list(data[data['OI Cloud Coverage'] <= 0.01]['Measurement Timestamp (UTC)'].unique())"],"id":"bFlZJByNbvUH"},{"cell_type":"markdown","source":["---\n","# Functions"],"metadata":{"id":"survpJiqVxJb"},"id":"survpJiqVxJb"},{"cell_type":"markdown","metadata":{"id":"y3kPOwzkbvUI"},"source":["### Extract training and validation data (geometries & detections)"],"id":"y3kPOwzkbvUI"},{"cell_type":"markdown","source":["The following cells assume all of the AOI and detections shapefiles (cloud_cover and detections) are stored in a directory called `aoi_data_by_time`. Within this directory, they are sorted into folders named with the date that an observation occurs, and the final directory is the name of the AOI at which the data is recorded."],"metadata":{"id":"XbOzVCyrBIGC"},"id":"XbOzVCyrBIGC"},{"cell_type":"code","execution_count":null,"metadata":{"id":"O8kpNnV4bvUJ"},"outputs":[],"source":["def get_cloud(aoi, date_list):\n","    \"\"\"\n","    Getting all the cloud geometry on `date_list` for `aoi` \n","\n","    Method to import all the cloud geometries from local directory to accelerate workflow\n","    Args:\n","    - date_list : list of string dates of interest in YYYY-MM-DD format\n","    - aoi : a single AOI of obesrvations to pull from\n","\n","    Returns: GeoDataFrame object\n","    \"\"\"\n","    all_cloud_list = []\n","    all_cloud_date = []\n","    aoi_data_by_time_dir = \"aoi_data_by_time \" +  aoi\n","    aoi_of_interest = aoi\n","    for date_dir in tqdm.tqdm(os.listdir(aoi_data_by_time_dir)):\n","      if date_dir in date_list:\n","        for aoi_dir in os.listdir(aoi_data_by_time_dir + \"/\" + date_dir):\n","            if aoi_dir == aoi_of_interest:\n","                for file in os.listdir(aoi_data_by_time_dir + \"/\" + date_dir + \"/\" + aoi_dir):\n","                    if file == \"cloud_cover.shp\":\n","                      all_cloud_list.append(gpd.read_file(aoi_data_by_time_dir + \"/\" + date_dir + \"/\" + aoi_dir + \"/\" + file))\n","                      all_cloud_date.append(pd.Series(data = [date_dir], index = ['date']))\n","                      all_cloud_date.append(pd.Series(data = [date_dir], index = ['date']))\n","                      all_cloud_date.append(pd.Series(data = [date_dir], index = ['date']))\n","    all_cloud_Aktubinsk = gpd.GeoDataFrame(pd.concat(all_cloud_list, ignore_index = True))\n","    all_cloud = gpd.GeoDataFrame(pd.concat(all_cloud_date, ignore_index=True)).rename(columns={0: \"date\"})\n","    cloud_data = all_cloud_Aktubinsk.join(all_cloud).reset_index()\n","    return cloud_data"],"id":"O8kpNnV4bvUJ"},{"cell_type":"code","execution_count":null,"metadata":{"id":"3nT898JUbvUK"},"outputs":[],"source":["def get_detection(aoi, date_list):\n","    \"\"\"\n","    Getting all the detections on `date_list` for `aoi` \n","\n","    Method to import all the detections from local directory to accelerate workflow\n","    Args:\n","    - date_list : list of string dates of interest in YYYY-MM-DD format\n","    - aoi : a single AOI of obesrvations to pull from\n","\n","    Returns: GeoDataFrame object\n","    \"\"\"\n","    all_detections_list = []\n","    aoi_data_by_time_dir = \"aoi_data_by_time \" + aoi\n","    all_date = []\n","    aoi_of_interest = aoi\n","    for date_dir in tqdm.tqdm(os.listdir(aoi_data_by_time_dir)):\n","        if date_dir in date_list:\n","            for aoi_dir in os.listdir(aoi_data_by_time_dir + \"/\" + date_dir):\n","                if aoi_dir == aoi_of_interest:\n","                        for file in os.listdir(aoi_data_by_time_dir + \"/\" + date_dir + \"/\" + aoi_dir):\n","                            if file == \"detections.shp\":\n","                                f = gpd.read_file(aoi_data_by_time_dir + \"/\" + date_dir + \"/\" + aoi_dir + \"/\" + file)\n","                                f['date'] = date_dir\n","                                all_detections_list.append(f)\n","    return gpd.GeoDataFrame(pd.concat(all_detections_list, ignore_index=True))"],"id":"3nT898JUbvUK"},{"cell_type":"code","execution_count":null,"id":"dd921c63","metadata":{"id":"dd921c63"},"outputs":[],"source":["# get training data\n","\n","akt_cloud_data_tr = get_cloud('Aktubinsk', train['Aktubinsk'])\n","akt_det_tr = get_detection('Aktubinsk', train['Aktubinsk'])\n","\n","mis_cloud_data_tr = get_cloud('Mistrata', train['Misrata Airport'])\n","mis_det_tr = get_detection('Mistrata', train['Misrata Airport'])\n","\n","kac_cloud_data_tr = get_cloud('Kacha', train['Kacha airfield'])\n","kac_det_tr = get_detection('Kacha', train['Kacha airfield'])\n","\n","tol_cloud_data_tr = get_cloud('Tolmacheva', train['Tolmacheva'])\n","tol_det_tr = get_detection('Tolmacheva', train['Tolmacheva'])\n","\n","al_cloud_data_tr = get_cloud('Al-Khadim Air Base', train['Al-Khadim Air Base'])\n","al_det_tr = get_detection('Al-Khadim Air Base', train['Al-Khadim Air Base'])"]},{"cell_type":"markdown","id":"-stFbTXC7ogJ","metadata":{"id":"-stFbTXC7ogJ"},"source":["### Generate random cloud cover\n"]},{"cell_type":"code","execution_count":null,"id":"fAh6ZTuP843z","metadata":{"id":"fAh6ZTuP843z"},"outputs":[],"source":["def random_points_within(poly, num_points):\n","  \"\"\"\n","  Generate `n` points for the random cloud on AOI `n`\n","\n","  Args:\n","  - poly : a polygon object\n","  - num_points : the number of points from which to construct the cloud\n","\n","  Returns: Point Geometry Object\n","  \"\"\"\n","  min_x, min_y, max_x, max_y = poly.total_bounds\n","  points = []\n","  while len(points) < num_points:\n","      random_point = Point([random.uniform(min_x, max_x), random.uniform(min_y, max_y)])\n","      points.append(random_point)\n","  return points"]},{"cell_type":"code","execution_count":null,"id":"85v7WaUuUEgD","metadata":{"id":"85v7WaUuUEgD"},"outputs":[],"source":["# dictionary storing AOI names to IDs\n","aoi_name_to_id = {\"Aktubinsk\": \"e9f9c924-a445-47b8-9e22-8dd802936a89\",\n","                    \"Zhukovskiy\": \"bc3eff13-8a86-486d-aa24-ada1f90e1a8a\",\n","                    \"Kacha airfield\": \"2d892c22-93b7-4fd7-840a-3d8c1172e797\",\n","                    \"Tolmacheva\": \"714b9bb1-a9e4-44ca-be7c-ff4069d52355\",\n","                    \"Al Jufra Airbase\": \"56ceb71e-231a-4df5-8035-17bd1f437db8\",\n","                    \"Al-Khadim Air Base\": \"b496fd94-d64e-4d67-996e-0d0cfb6a5080\",\n","                    \"Misrata Airport\": \"21a34ce3-360d-488c-93af-144696257acc\"}\n","\n","\n","def aoi_with_rand_cc(list_of_aoi, list_of_dates, num_points, all_clouds=akt_cloud_data_tr):  \n","  \"\"\"\n","  Function to generate random cloud covers for inputted AOIs on inputted dates.\n","  Inputs:\n","    - list_of_aoi: a list of aoi names (list of strings)\n","    - list_of_dates: a list of dates to generate the random cloud covers (list of strings)\n","    - num_points: the number of points our randomly generated cloud cover should have\n","    - all_clouds: a geodataframe of all the clouds (for the specified aoi)\n","  \n","  Returns: `aois` GeoDataFrame containing polygons for each of AOI, Clouds, Clear, and Random (our generated cloud cover)\n","  \"\"\"\n","\n","  AOIs = list_of_aoi # INPUT string list of AOI names\n","  date_list = list_of_dates # INPUT string list of dates\n","  \n","  # convert AOI names to IDs\n","  aoi_id_to_name = {}\n","  aois_shply = []\n","  for key in aoi_name_to_id:\n","      aoi_id_to_name[aoi_name_to_id[key]] = key\n","  aoi_ids = [aoi_name_to_id[AOI] for AOI in AOIs]\n","\n","  aois = all_clouds[all_clouds['date'] == list_of_dates[0]].drop(columns=['index']) # Pull the Geodataframe for the specific aoi on the specific day\n","  # Below is used to manually derive \"clear\" since it is often None when imported\n","  \n","  # Uses the existing polygon to create a cloud over which we have data\n","  poly = aois[aois['aoi_id'] == 'AOI']['geometry'].iloc[0]\n","\n","  # if no clouds, then the clear region is the whole AOI\n","  if aois[aois['aoi_id'] == 'Clouds']['geometry'].iloc[0]:\n","    poly = poly.difference(aois[aois['aoi_id'] == 'Clouds']['geometry'].iloc[0].buffer(0))\n","  full_poly = aois[aois['aoi_id'] == 'AOI']['geometry'] # polygon of the entire AOI\n","  points = random_points_within(gpd.GeoSeries(poly), num_points)\n","  rand_poly = Polygon([[p.x, p.y] for p in points]) #  Ensures that the randomly generated polygon is contained within the entire AOI\n","  \n","  if len(aois.index) == 4:\n","      aois['geometry'].iloc[3] = rand_poly # overwrites an existing random cloud\n","  else:\n","      aois.loc[len(aois.index)] = ['Random', rand_poly, list_of_dates[0]] # else adds it to the Geodataframe\n","  return aois"]},{"cell_type":"markdown","id":"HIRPrq8insxf","metadata":{"id":"HIRPrq8insxf"},"source":["### Generate the Fishnet Grid on AOI"]},{"cell_type":"code","execution_count":null,"id":"-g56Pwsz80pH","metadata":{"id":"-g56Pwsz80pH"},"outputs":[],"source":["def fishnetgrid(list_of_aoi, det_tr, all_clouds, list_of_dates, n_cells=20):\n","  \"\"\"\n","  Generates a fishnet grid over the AOIs\n","  Also converts detections from polygon objects into single points at their centroid\n","  \n","  Args:\n","  - list_of_aoi : a string list of AOI names\n","  - det_tr : the detections that are part of the training set\n","  - all_clouds : the data frame of clouds for all the days and AOIs\n","  - list_of_dates : a string list of dates (YYYY-MM-DD HH:MM:SSZ)\n","\n","  Options:\n","  - n_cells : number of cells to divide the grid into\n","\n","  Returns the grid geodataframe and detections geodataframe\n","  \"\"\"\n","  # AOI DF FOR A CERTAIN DATE AND TIME\n","  aois = all_clouds[all_clouds['date'] == list_of_dates[0]]\n","  poly = aois['geometry'].iloc[0]\n","  xmin, ymin, xmax, ymax = aois.iloc[[0],:].total_bounds\n","\n","  # projection of the grid\n","  crs = \"+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +a=6371007.181 +b=6371007.181 +units=m +no_defs\"\n","\n","  cell_size = (xmax-xmin)/n_cells\n","  n_cells_y = np.ceil((ymax-ymin)/cell_size)\n","\n","  # create the cells in a loop\n","  grid_cells = []\n","  # Defining Grid Index\n","  id = []\n","  for i in np.arange(0, n_cells, 1):\n","      for j in np.arange(0, n_cells_y+1, 1):\n","          # bounds\n","          x1 = xmin+(cell_size*i)\n","          y1 = ymin+(cell_size*j)\n","          # Appending to the Grid Index at each iteration\n","          id.append(np.array([i,j]))\n","          # Appending to the Grid Cells at each iteration\n","          grid_cells.append(shapely.geometry.box(x1+cell_size, y1-cell_size, x1, y1)  )\n","  id = np.array(id)\n","  idgrid = [i for i in range(len(grid_cells))]\n","  gridcells = gpd.GeoDataFrame({\"id\":idgrid,\"idx\":id[:,0],\"idy\":id[:,1],\"geometry\":grid_cells}, crs=crs)\n","  grid = gridcells[['geometry']]\n","  # Change detections geodataframe to be centroid points instead of polygons\n","  det_gdf = det_tr[det_tr['date'] == list_of_dates[0]]['geometry'].centroid #['detections'].iloc[0]['geometry'].centroid\n","  return grid, det_gdf\n","\n","def add_cor(grid):\n","  '''\n","  Function to add coordinates to grid\n","  \n","  Inputs:\n","  - grid: geodataframe of grid, containing a row for each grid cell, represented by a polygon\n","  \n","  Returns: grid geodataframe with the added column `cor` that is the centroid coordinate of each grid cell\n","  '''\n","  grid.reset_index(drop=True, inplace=True)\n","\n","  geom = grid['geometry'] # accesses grid geometry\n","  cor = [] # initializes coordinates list\n","\n","  # stores the 0th gridcell centroid coordinate\n","  x = geom[0].centroid.x\n","  y = geom[0].centroid.y\n","  temp = [x,y]\n","  cor.append(temp)\n","\n","  # iterate over the remaining centroids of each gridcell\n","  for i in range(1, len(geom)):\n","      \n","      x = geom[i].centroid.x\n","      y = geom[i].centroid.y\n","      temp_a = [x,y]\n","      \n","      cor.append(temp_a)\n","      \n","  grid['cor'] = cor # adds column to gdf\n","  return grid"]},{"cell_type":"markdown","id":"lDknPDxuJm88","metadata":{"id":"lDknPDxuJm88"},"source":["### Prediction"]},{"cell_type":"markdown","source":["Adapted from the simple kriging approach at http://edzer.github.io/mstp/lec5.html\n","\n","\n","1) `get_prediction` is for a specified latitude & longitude (i.e. for a single gridcell)\n","\n","2) `get_prediction_grid` is for all the grids in a grid matrix\n","\n","3) `integrated_prediction` is the function that runs the entire pipeline. This takes in an AOI, the corresponding geometry files, and the date of relevance in order to generate a prediction for each gridcell and return objects that can be plotted in the heatmap"],"metadata":{"id":"_3XFrPzLzc72"},"id":"_3XFrPzLzc72"},{"cell_type":"code","execution_count":null,"id":"NWX9VPRBrS51","metadata":{"id":"NWX9VPRBrS51"},"outputs":[],"source":["def get_prediction(grid_matrix, location_lat, location_lon, arr_vehicle):\n","  '''\n","  Function to get a prediction (through Kriging) for a specified latitude and longitude\n","  \n","\n","  Inputs:\n","    - grid_matrix: geodataframe of grids, represented in each row by a polygon\n","    - location_lat: latitude\n","    - location_lon: longitude\n","    - arr_vehicle: List of number of vehicles in each grid\n","  '''\n","  # for one point with other all other points at the same time \n","  a = []\n","  all_matrices = [] \n","  for i in range(len(grid_matrix)):\n","      a.append([grid_matrix['cor'][i], [location_lat, location_lon]]) # input    \n","  for it in range(len(a)):\n","      matrix = np.exp(np.negative(dt.euclidean(a[it][0],a[it][1])))\n","      all_matrices.append(matrix)\n","  covariance_matrix = all_matrices\n","  covariance_matrix_trans = np.array(covariance_matrix).T\n","\n","  mu = np.mean(arr_vehicle)\n","  Z_minus_mu = arr_vehicle - mu\n","\n","  a = []\n","  w = len(grid_matrix)\n","  h = len(grid_matrix)\n","  variance = [[0 for x in range(w)] for y in range(h)] \n","  for i in range(len(grid_matrix)):\n","      for j in range(len(grid_matrix)):\n","          var = np.exp(np.negative(dt.euclidean(grid_matrix['cor'][i], grid_matrix['cor'][j])))\n","          variance[i][j] = var\n","\n","  V_inverse = np.linalg.inv(variance)\n","\n","  v = all_matrices\n","  pre = mu + covariance_matrix_trans @ V_inverse @ Z_minus_mu\n","  return pre"]},{"cell_type":"code","execution_count":null,"id":"PbM2Sj8KGFTd","metadata":{"id":"PbM2Sj8KGFTd"},"outputs":[],"source":["def get_prediction_grid(grid_matrix, prediction_grid, arr_vehicle):\n","    '''\n","    Function to get preductions for all grids in a grid matrix\n","    Inputs:\n","      - grid_matrix: gdf of grids, represented in each row by a polygon\n","      - prediction grid: grids in which we are intersted in seeing the predictions of\n","      - arr_vehicle: List of number of vehicles in each grid\n","    '''\n","    a = []\n","    for i in range(len(prediction_grid)):\n","        x = prediction_grid[i][0]\n","        y = prediction_grid[i][1]\n","        a.append(get_prediction(grid_matrix, x, y , arr_vehicle))\n","    return a\n"]},{"cell_type":"markdown","source":["This is the function where the `validate` parameter (default = `True`) will generate a random cloud on the input data."],"metadata":{"id":"aNXUt-N-aGiJ"},"id":"aNXUt-N-aGiJ"},{"cell_type":"code","execution_count":null,"id":"1d1MHrW3ypDx","metadata":{"id":"1d1MHrW3ypDx"},"outputs":[],"source":["def integrated_prediction(all_clouds = akt_cloud_data_tr, det_tr = akt_det_tr, AOI = 'Aktubinsk', date ='2020-02-09 08:11:37Z', \n","                          num_cells = 30, validate = True, num_rand = 3):\n","    # Integrated code for 1 AOI and 1 date\n","    \"\"\"\n","    Runs all the above functions to make predictions by gridcell on the AOI\n","    1. Divdes the AOI into gridcells\n","    2. Generates a random cloud if validation=True\n","    3. Computes Kriging prediction for each gridcell\n","    4. Returns predictions\n","\n","    Options:\n","    - all_clouds : cloud training geodataframe for an AOI (default = akt_cloud_data_tr)\n","    - det_tr : corresponding detections training geodataframe for the AOI (default = akt_det_tr)\n","    - AOI : string name for the AOI (default = Aktubinsk)\n","    - date : corresponding string date for which the clouds/detections are observed (default = 2020-02-09 08:11:37Z)\n","    - num_cells : number of grid cells to divide the AOI into\n","    - num_rand : number of points to generate a random cloud for validation\n","    - validate : boolean determining if a random cloud should be generated and predicted for validation (default  = True)\n","\n","    Returns:\n","    - grid : fishnet gridcells applied to AOI geometry\n","    - det_gdf : detections converted to centroid points rather than polygons\n","    - rand_poly : the geodataframe of geometries including AOI, Clear, Clouds, Random (if validate=True)\n","    - al_pred_rand_cloud : predicted + baseline (visible) counts for each gridcell\n","    - arr_vehicle : actual observed counts for each gridcell\n","    - arr_vehicle_reduced : non-cloudy observed counts for each gridcell (removing cloudy gridcell indices)\n","    - arr_vehicle_rand : non-cloudy observed counts for each gridcell (zeroing cloudy gridcell indices) -- this is the baseline (visible) counts\n","    - pred_rand_cloud : predicted counts for cloudy gridcells only\n","    \"\"\"\n","\n","    # applies the fishnet grid\n","    grid, det_gdf = fishnetgrid([AOI], det_tr = det_tr, all_clouds = all_clouds, list_of_dates = [date], n_cells = num_cells)\n","\n","    # removes the gridcells that are outside of the AOI\n","    aois = all_clouds[all_clouds['date'] == [date][0]]\n","    poly = aois['geometry'].iloc[0]\n","    indexes_to_remove = []\n","    for i in range(len(grid)):\n","        if not grid.iloc[i]['geometry'].intersects(poly):\n","            indexes_to_remove.append(i)\n","    grid = grid.drop(grid.index[indexes_to_remove]) # drop the grids that are not touching the aoi at all \n","\n","    # OPTIONAL: applies a random cloud if VALIDATE = TRUE\n","    if validate:\n","      rand_poly = aoi_with_rand_cc([AOI], [date], num_rand, all_clouds)\n","      rand_cloud_cover = rand_poly['geometry'].iloc[3]\n","      max_cloud_percent = .30\n","      min_cloud_percent = .20\n","      cloud_percent = rand_cloud_cover.area / poly.area\n","      while cloud_percent > max_cloud_percent or cloud_percent < min_cloud_percent: # Generate a random cloud and ensure that it is covering less than 30% of the aoi\n","          rand_poly = aoi_with_rand_cc([AOI], [date], num_rand,all_clouds)\n","          rand_cloud_cover = rand_poly['geometry'].iloc[3]\n","          cloud_percent = rand_cloud_cover.area / poly.area\n","\n","    else:\n","          # uses real cloud if VALIDATE = FALSE\n","          rand_poly = aois\n","          rand_cloud_cover = rand_poly.iloc[0]['geometry']\n","      \n","    new_poly = poly.difference(rand_cloud_cover) # aoi polygon subtracting the cloud cover\n","\n","    reduced_grid = grid.copy()\n","    # Removing grids that were within the cloud cover\n","    indexes_to_remove_random_cloud = []\n","    for i in range(len(reduced_grid)):\n","        if rand_cloud_cover.contains(reduced_grid.iloc[i]['geometry']):\n","            indexes_to_remove_random_cloud.append(i)\n","    reduced_grid = reduced_grid.drop(reduced_grid.index[indexes_to_remove_random_cloud]) # drop the grids that are fully within the cloud cover\n","\n","    arr_vehicle = [] # nx1 matrix of detections in all grids - this array contains a count of detected objects for every grid\n","    # this is to validate total predictions\n","    arr_vehicle_rand = [] # nx1 matrix of detections that do not fall under the cloud - this array contains a count of detected objects for every non-cloud grid\n","    # arr_vehicle_rand will be equal to arr_vehicle if validate = False because we will have predictions underneath an artificially generated cloud\n","    # aka this variable will be our baseline (visible) counts\n","    for polygon in grid['geometry']:\n","        veh_loc = 0\n","        veh_rand_loc = 0\n","        veh_rand_reduced = 0\n","        for heli in det_gdf:\n","            if heli.within(polygon):\n","                veh_loc += 1\n","                if not heli.within(rand_cloud_cover):\n","                    veh_rand_loc += 1 \n","        arr_vehicle.append(veh_loc)\n","        arr_vehicle_rand.append(veh_rand_loc)\n","\n","    arr_vehicle_reduced = [] # this is the same as arr_vehicle_rand (i.e. count of detected objects for non-cloud gridcells) except a lower dimension because\n","    # instead of zeroing out cloudy grid cells, it just skips over them\n","    for polygon in reduced_grid['geometry']:\n","        veh_rand_reduced = 0\n","        for heli in det_gdf:\n","            if heli.within(polygon) and not heli.within(rand_cloud_cover):\n","                veh_rand_reduced += 1\n","        arr_vehicle_reduced.append(veh_rand_reduced)\n","\n","    # add coordinates to grid and reduced_grid (cloud-less gridcells)\n","    grid = add_cor(grid)\n","    reduced_grid = add_cor(reduced_grid)\n","\n","    pred_rand_cloud = [] # list of predictions within cells intersecting clouds\n","    al_pred_rand_cloud = arr_vehicle.copy() # list of predictions along with the baseline (visible) counts\n","\n","    # Getting predictions for grids overlapping the clouded area only\n","    for i, polygon in grid[grid['geometry'].intersects(rand_cloud_cover)]['geometry'].iteritems():\n","        var = get_prediction(reduced_grid, polygon.centroid.x, polygon.centroid.y, arr_vehicle_reduced)\n","        pred_rand_cloud.append(var)\n","        al_pred_rand_cloud[i] = var\n","\n","    # Round predictions to 3 decimal places\n","    pred_rand_cloud = [round(i,3) for i in pred_rand_cloud]\n","    al_pred_rand_cloud = [round(i,3) for i in al_pred_rand_cloud]\n","\n","    return grid, det_gdf, rand_poly, al_pred_rand_cloud, arr_vehicle, arr_vehicle_reduced, arr_vehicle_rand, pred_rand_cloud"]},{"cell_type":"markdown","source":["# Make predictions"],"metadata":{"id":"EQcKuz9qV75Z"},"id":"EQcKuz9qV75Z"},{"cell_type":"code","source":["# In this example, we use Aktubinsk\n","AOI_name = 'Aktubinsk'\n","detections_df = akt_det_tr\n","geometries_df = akt_cloud_data_tr\n","generate_clouds = True # set this to false for unseen data (i.e. to not validate)"],"metadata":{"id":"-qLrDc3XWYpa"},"id":"-qLrDc3XWYpa","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"e97b528b","metadata":{"id":"e97b528b"},"outputs":[],"source":["pred_list = []\n","counter = 0\n","\n","for date in tqdm.tqdm(np.unique(detections_df['date'].values)):\n","  grid, det_gdf, rand_poly, al_pred_rand_cloud, arr_vehicle, arr_vehicle_reduced, arr_vehicle_rand, pred_rand_cloud = integrated_prediction(all_clouds = geometries_df, \n","                                                                                                                                            det_tr = detections_df, AOI = AOI_name, \n","                                                                                                                                            date = date, num_cells = 30, num_rand = 3,\n","                                                                                                                                            validate = generate_clouds)\n","  # pred_list contains [date, actual observed counts, predicted counts = baseline + predictions, baseline (visible) counts]\n","  pred_list.append([date, sum(arr_vehicle), round(sum(al_pred_rand_cloud)), sum(arr_vehicle_rand)])\n","\n","pred_list[:5]\n","    "]},{"cell_type":"markdown","source":["## Prediction metrics and visualization"],"metadata":{"id":"dCyH1PdZHzMf"},"id":"dCyH1PdZHzMf"},{"cell_type":"code","source":["def get_mape(y_true, y_pred):\n","  \"\"\"\n","  Return the Mean Absolute Percentage Error (MAPE) for a given set of predictions\n","  Args:\n","  - y_true : the true counts of detected objects\n","  - y_pred : the predicted counts of detected objects\n","  Output: float\n","  \"\"\"\n","  mape = mean_absolute_percentage_error(y_true, y_pred)\n","  return mape"],"metadata":{"id":"FDTckLpvWbwb"},"id":"FDTckLpvWbwb","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"ea346489","metadata":{"id":"ea346489"},"outputs":[],"source":["### Generating Mean Absolute Percentage Error for each AOI across all days.\n","get_mape([a[1] for a in pred_list], [a[2] for a in pred_list])"]},{"cell_type":"code","source":["def timeseries_pred_plot(AOI_name, pred_list, detections_df):\n","  \"\"\"  \n","  Prints out the Mean Absolute Percentage Error (MAPE) for a set of predictions\n","  and also generates a time series plot of the actual, predicted, and visible\n","  detections over a set of dates \n","\n","  Args:\n","    - pred_list : a 4D array with columns [dates, actual_counts, predicted_counts, visible_counts]\n","\n","  Output: None, shows Matplotlib figure\n","  \"\"\"\n","  y_true = [a[1] for a in pred_list]\n","  y_pred = [a[2] for a in pred_list]\n","  y_base = [a[3] for a in pred_list]\n","  dates = [a[0] for a in pred_list]\n","\n","  mape = get_mape(y_true, y_pred)\n","  print(f'Mean absolute percentage error: {mape}')\n","\n","  plt.figure(figsize=(10, 5))\n","  plt.plot(dates, y_true, label = \"actual detections\")\n","  plt.plot(dates, y_pred, label = \"predicted detections\")\n","  plt.plot(dates, y_base, label = \"baseline (visible)\", color='green')\n","  plt.xticks(rotation=90)\n","  plt.legend()\n","  plt.xticks(\" \")\n","\n","  date_end = max(detections_df['date'].values)\n","  date_start = min(detections_df['date'].values)\n","\n","  plt.title(f\"Kriging Prediction Plots for {AOI_name} from {date_start} to {date_end}\")\n","  plt.show()"],"metadata":{"id":"iickgq3IWvYY"},"id":"iickgq3IWvYY","execution_count":null,"outputs":[]},{"cell_type":"code","source":["timeseries_pred_plot(AOI_name, pred_list, detections_df)"],"metadata":{"id":"z1vgrVOdadEK"},"id":"z1vgrVOdadEK","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The green line (visible) line will be identical to the observed (blue) line if no random cloud was generated."],"metadata":{"id":"957e8RInaqDN"},"id":"957e8RInaqDN"},{"cell_type":"code","execution_count":null,"id":"PAqqRusqCDZH","metadata":{"id":"PAqqRusqCDZH"},"outputs":[],"source":["### sanity checks\n","# arr_vehicle: List of detections for all grids in AOI\n","# arr_vehicle_rand: List of detections for all grids in AOI that aren't covered by randomly generated cloud\n","# arr_vehicle_reduced: List of detections for not-completely-cloud-covered grids in AOI whose detections aren't covered by cloud\n","###\n","\n","print(\"sum arr_vehicle_reduced:\", sum(arr_vehicle_reduced))\n","print(\"sum arr_vehicle:\", sum(arr_vehicle))\n","print(\"sum arr_vehicle_rand:\", sum(arr_vehicle_rand))\n","print(\"len arr_vehicle_reduced:\", len(arr_vehicle_reduced))\n","print(\"len arr_vehicle:\", len(arr_vehicle))\n","print(\"len arr_vehicle_rand:\", len(arr_vehicle_rand))"]},{"cell_type":"code","execution_count":null,"id":"ToCP3Fo-r9CZ","metadata":{"id":"ToCP3Fo-r9CZ"},"outputs":[],"source":["# Comparing kriging detections to actual detections\n","import itertools\n","import operator\n","\n","print(\"Correct number of detections in generated cloud cover:\", sum(arr_vehicle))\n","print(\"Predicted counts in generated cloud cover:\", sum(al_pred_rand_cloud))\n","print(\"Baseline counts which were visible:\", sum(arr_vehicle_rand))"]},{"cell_type":"markdown","id":"umLq__nW18oI","metadata":{"id":"umLq__nW18oI"},"source":["## Plotting into a heatmap "]},{"cell_type":"code","source":["def heatmap(grid, det_gdf, rand_poly, al_pred_rand_cloud, arr_vehicle, arr_vehicle_reduced, \n","            arr_vehicle_rand, pred_rand_cloud,\n","            AOI_name, geometries_df, detections_df):\n","    \"\"\"\n","    Plots a heatmap overalying the actual and predicted counts onto the AOI, including cloud cover\n","\n","    Args:\n","    The arguments are all the outputs of the `integrated_prediction` function along with some\n","\n","    (`integrated_prediction` function outputs)\n","    - grid : fishnet gridcells applied to AOI geometry\n","    - det_gdf : detections converted to centroid points rather than polygons\n","    - rand_poly : the geodataframe of geometries including AOI, Clear, Clouds, Random (if validate=True)\n","    - al_pred_rand_cloud : predicted + baseline (visible) counts for each gridcell\n","    - arr_vehicle : actual observed counts for each gridcell\n","    - arr_vehicle_reduced : non-cloudy observed counts for each gridcell (removing cloudy gridcell indices)\n","    - arr_vehicle_rand : non-cloudy observed counts for each gridcell (zeroing cloudy gridcell indices) -- this is the baseline (visible) counts\n","    - pred_rand_cloud : predicted counts for cloudy gridcells only\n","\n","    (other inputs)\n","    - AOI_name : string name of AOI\n","    - geometries_df : the geodataframe with all the geometries for the AOI across all the training dates\n","    - detections_df : the geodataframe wtih all the detections for the AOI across all the training dates\n","\n","    Output: None, shows Matplotlib figure\n","    \"\"\"\n","    # Plotting output into a heat map for a specific date and AOI\n","\n","    prediction_grid = grid['cor']\n","    grid['Prediction_all'] = al_pred_rand_cloud\n","    grid['Detected_hidden_by_rand_cloud'] = arr_vehicle_rand\n","    grid['Detected'] = arr_vehicle\n","    grid['Detected_hidden_by_rand_cloud'] = grid['Detected_hidden_by_rand_cloud'].astype(np.float64)\n","    grid['Detected'] = grid['Detected'].astype(np.float64)\n","\n","    # plot the cloud\n","    cloud_gdf = rand_poly[rand_poly['aoi_id'] == 'Clear']\n","    random_gdf = rand_poly[rand_poly['aoi_id'] == 'Random'] # if validate = True\n","\n","    aois = geometries_df[geometries_df['date'] == detections_df['date'].values[0]]\n","    poly = [aois['geometry'].iloc[0]]\n","    type(poly)\n","    poly_gdf = gpd.GeoDataFrame(poly)\n","    poly_gdf = poly_gdf.rename({0:'geometry'}, axis = 1)\n","\n","\n","    # heatmap\n","    base = grid.plot(figsize=(30, 10), alpha=0.5, legend = True, edgecolor = 'black', column='Prediction_all', scheme= 'quantiles', k=3, cmap='Blues')\n","    poly_gdf.plot(ax= base, edgecolor = 'black', color = 'yellow', alpha=0.2)\n","    cloud_gdf.plot(ax= base, color = 'red', alpha = 0.3)\n","    random_gdf.plot(ax = base, color = 'green', alpha = 0.3) # if validate = True\n","    plt.title(f'Heatmap for predictions on AOI {AOI_name}')\n","    plt.xlabel(\"Longitude\")\n","    plt.ylabel(\"Latitude\")\n","\n","    for i in range(len(grid)):\n","        xy_val = tuple(grid['cor'][i])\n","        value = round(grid['Prediction_all'][i])\n","        plt.annotate(value, xy=xy_val, color = 'green', xytext=(xy_val[0]+0.00005, xy_val[1]+0.0005), fontsize = 8)\n","        xy_detected_cloud = grid['Detected_hidden_by_rand_cloud'][i]\n","        xy_detected = grid['Detected'][i]\n","        plt.annotate(xy_detected, xy=xy_val, color = 'black', xytext=(xy_val[0]-0.0003, xy_val[1]-0.0003), fontsize = 8)\n","\n","\n","    from matplotlib.lines import Line2D\n","    custom_lines = [Line2D([0], [0], color='black', lw=4),\n","                    Line2D([0], [0], color='green', lw=4)]\n","    plt.legend(custom_lines, ['Actual counts', 'Predicted'])\n","\n","    plt.show()"],"metadata":{"id":"pxZDCPn8YgN8"},"id":"pxZDCPn8YgN8","execution_count":null,"outputs":[]},{"cell_type":"code","source":["heatmap(grid, det_gdf, rand_poly, al_pred_rand_cloud, arr_vehicle, arr_vehicle_reduced, \n","            arr_vehicle_rand, pred_rand_cloud,\n","            AOI_name, geometries_df, detections_df)"],"metadata":{"id":"pzBr2do5agGh"},"id":"pzBr2do5agGh","execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Kriging.ipynb","provenance":[{"file_id":"1oCC8Uxd6Lo-mOows-InlLqtCGb_P_sS7","timestamp":1638322395577},{"file_id":"1tpmqTGet3q60FQuht__oGy7FkLK5aoSo","timestamp":1637611315992}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}